{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Names of group members (max 3):\n",
    "    \n",
    "    Dmitri Shostakovich\n",
    "    Gustav Mahler\n",
    "    Johannes Brahms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $$\\mbox{Logistic Regression.}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../standard_import.txt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "pd.set_option('display.notebook_repr_html', False)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 150)\n",
    "pd.set_option('display.max_seq_items', None)\n",
    " \n",
    "#%config InlineBackend.figure_formats = {'pdf',}\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loaddata(file, delimeter):\n",
    "    data = np.loadtxt(file, delimiter=delimeter)\n",
    "    print('Dimensions: ',data.shape)\n",
    "    print(data[1:6,:])\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData(data, label_x, label_y, label_pos, label_neg, axes=None):\n",
    "    ## the data gives the grades for two exams; \n",
    "    ## the last entry in the data is a 0 or 1: \n",
    "    ##   rejected or accepted for a certain programme\n",
    "    ## make a scatter plot of the data such that \n",
    "    ## a 1 gets a black plus, the 0 a yellow (not too) large dot\n",
    "    if axes == None:\n",
    "        axes = plt.gca()\n",
    "\n",
    "        ## ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$\\mbox{Logistic regression.}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $$ \\mbox{Logistic regression hypothesis:} \\hspace{1cm} h_{\\theta}(x) = \\sigma(\\theta^{T}x), ~~~ \\sigma(z)=\\frac{1}{1+e^{âˆ’z}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    ### return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $$\\mbox{Prediction.}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta, X, threshold = 0.5):\n",
    "    ### return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  $$\\mbox{Regularized logistic regression.}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = loaddata('data/ex2data2.txt', ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## y = \n",
    "## X ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotData(data2, 'Microchip Test 1', 'Microchip Test 2', 'y = 1', 'y = 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   $$\\mbox{Polynomials.}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that this function inserts a column with 'ones' in the design matrix for the intercept.\n",
    "poly = PolynomialFeatures(6)\n",
    "XX = poly.fit_transform(data2[:,0:2])\n",
    "XX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $$\\mbox{Regularized Cost Function:}\\hspace{1cm} J(\\theta) = -\\frac{1}{m}\\big((\\,\\ln\\,(\\sigma(X\\theta))^Ty+(\\,\\ln\\,(1-\\sigma(X\\theta))^T(1-y)\\big) + \\frac{\\lambda_{reg}}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunctionReg(theta, X, y, reg):\n",
    "    ### J = ...\n",
    "    if np.isnan(J):\n",
    "        return(np.inf)\n",
    "    return(J)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $$\\mbox{Partial derivative:}\\hspace{1cm}  \\frac{\\partial J(\\theta)}{\\partial\\theta_{j}} = \\frac{1}{m} X^T(\\sigma(X\\theta)-y) + \\frac{\\lambda_{reg}}{m}\\theta_{j}$$\n",
    "##### $$\\text{Note: intercept parameter } \\theta_{0} \\text{ is not to be regularized}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientReg(theta, X, y, reg):\n",
    "    ## return .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_theta = np.c_[np.zeros(XX.shape[1])]\n",
    "costFunctionReg(initial_theta, XX, y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentReg(theta, X, y, reg, alpha = 0.001, num_iters = 50000):\n",
    "    M = y.size\n",
    "    J = 0\n",
    "    for iter in np.arange(num_iters):\n",
    "        ## theta = ...\n",
    "        ## J = ...\n",
    "    return(theta, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we vary the value of the regulaization parameter, reg\n",
    "i = 0\n",
    "fig, axes = plt.subplots(1,3, sharey = True, figsize=(17,5))  \n",
    "for reg in {100, 1, 0.01}:\n",
    "    theta = initial_theta\n",
    "    print('initial cost: ',costFunctionReg(theta, XX, y, reg))\n",
    "    theta , J = gradientDescentReg(theta, XX, y, reg)\n",
    "    print('final cost: ',costFunctionReg(theta, XX, y, reg))\n",
    "    print('theta = ', theta.T)\n",
    "    accuracy = 100*(sum(predict(theta, XX) == y))/y.size\n",
    "    plotData(data2, 'Microchip Test 1', 'Microchip Test 2', 'y = 1', 'y = 0', axes.flatten()[i])\n",
    "    # Plot decisionboundary\n",
    "    x1_min, x1_max = X[:,0].min(), X[:,0].max(),\n",
    "    x2_min, x2_max = X[:,1].min(), X[:,1].max(),\n",
    "    xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\n",
    "    h = sigmoid(np.dot(poly.fit_transform(np.c_[xx1.ravel(), xx2.ravel()]), theta))\n",
    "    h = h.reshape(xx1.shape)\n",
    "    axes.flatten()[i].contour(xx1, xx2, h, [0.5], linewidths=1, colors='g'); \n",
    "    axes.flatten()[i].set_title('Train accuracy {}% with reg = {}'.format(np.round(accuracy[0], decimals=2), reg))\n",
    "    i = i + 1\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## what is your comment on these results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
