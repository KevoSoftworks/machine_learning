{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $$\\mbox{Logistic Regression with Gradient Descent.}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../standard_import.txt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "pd.set_option('display.notebook_repr_html', False)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 150)\n",
    "pd.set_option('display.max_seq_items', None)\n",
    " \n",
    "#%config InlineBackend.figure_formats = {'pdf',}\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loaddata(file, delimeter):\n",
    "    data = np.loadtxt(file, delimiter=delimeter)\n",
    "    print('Dimensions: ',data.shape)\n",
    "    print(data[1:6,:])\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData(data, label_x, label_y, label_pos, label_neg, axes=None):\n",
    "    ## the data gives the grades for two exams; \n",
    "    ## the last entry in the data is a 0 or 1: \n",
    "    ##   rejected or accepted for a certain programme\n",
    "    ## make a scatter plot of the data such that \n",
    "    ## a 1 gets a black plus, the 0 a yellow (not too) large dot\n",
    "    if axes == None:\n",
    "        axes = plt.gca()\n",
    "    ## ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$\\mbox{Logistic regression.}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loaddata('data/ex2data1.txt', ',')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## X: of the form [1 x1 x2] \n",
    "## y: 0's and 1's\n",
    "## X = ...\n",
    "## y = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotData(data, 'Exam 1 score', 'Exam 2 score', 'Admitted', 'Not admitted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $$ \\mbox{Logistic regression hypothesis:} \\hspace{1cm} h_{\\theta}(x) = \\sigma(\\theta^{T}x), ~~~ \\sigma(z)=\\frac{1}{1+e^{âˆ’z}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    ### return ...\n",
    "    \n",
    "## create a picture of this function on [-6, 6]\n",
    "## xxx = ...\n",
    "## yyy = ...\n",
    "## plt..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $$ {\\mbox{Cost function:}} \\hspace{1cm}     J(\\theta) = -\\frac{1}{m}\\big(\\,\\ln\\,(\\sigma(X\\theta))^Ty+\\ln\\,(1-\\sigma(X\\theta))^T(1-y)\\big)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(theta, X, y):\n",
    "    ## m = ...\n",
    "    ## J = ...\n",
    "    if np.isnan(J):\n",
    "        return(np.inf)\n",
    "    ## return ...\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $$ \\mbox{Partial derivative:}\\hspace{1cm}\\frac{\\partial J(\\theta)}{\\partial\\theta_{j}} = \\frac{1}{m} X^T(\\sigma(X\\theta)-y)$$\n",
    "\n",
    "Both the vector theta and the gradient should for this problem be (3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(theta, X, y):\n",
    "    ## m = ...\n",
    "    ## grad = ...\n",
    "    ## return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking the shapes and values\n",
    "initial_theta = np.c_[np.zeros(X.shape[1])] \n",
    "print('Shape theta: \\n', initial_theta.shape)\n",
    "cost = costFunction(initial_theta, X, y)\n",
    "grad = gradient(initial_theta, X, y)\n",
    "print('Initial cost: \\n', cost) \n",
    "print('Grad: \\n', grad.T)\n",
    "print('Shape grad: \\n', grad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $$\\mbox{Optimize cost function.}$$\n",
    "\n",
    "In GradientDescent (below) you have to play with the learning parameter alpha and the number of interations: you cannot tell what good values are. \n",
    "\n",
    "Keep the values for costfunction at intermediate steps in order to view the convergence.\n",
    "\n",
    "If this num_iters must be chosen very large (did the process converge?) you should write an extra loop within the loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X, y, theta, alpha = 0.001, numiters = 5500):\n",
    "    Jh = np.zeros(numiters)\n",
    "    for iter in np.arange(numiters):\n",
    "        ## theta = ...\n",
    "        ## Jh[...] = ...\n",
    "    return(theta, Jh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = initial_theta\n",
    "print('Initial cost: ',costFunction(theta, X, y))\n",
    "theta , Jh = gradientDescent(X, y, theta)\n",
    "print('Final cost: ',costFunction(theta, X, y))\n",
    "print('Values of theta: ',theta[0], theta[1])\n",
    "## plot the history of the cost function\n",
    "## put sensible text on axis\n",
    "## plt ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much from the whole data set using the optimized theta values from above has the correct classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta, X, threshold = 0.5):\n",
    "## ...\n",
    "\n",
    "p = predict(theta, X) \n",
    "    ## print ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student with Exam 1 score 45 and Exam 2 score 85: will he pass?\n",
    "## print(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(45, 85, s=60, c='r', marker='v', label='(45, 85)')\n",
    "plotData(data, 'Exam 1 score', 'Exam 2 score', 'Admitted', 'Not admitted')\n",
    "x1_min, x1_max = X[:,1].min(), X[:,1].max(),\n",
    "x2_min, x2_max = X[:,2].min(), X[:,2].max(),\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\n",
    "h = sigmoid(np.c_[np.ones((xx1.ravel().shape[0],1)), xx1.ravel(), xx2.ravel()].dot(theta))\n",
    "h = h.reshape(xx1.shape)\n",
    "plt.contour(xx1, xx2, h, [0.5], linewidths=1, colors='b');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
