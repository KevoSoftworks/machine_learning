{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Names of group members (max 2):\n",
    "    \n",
    "    Don Quixote\n",
    "    Caballo Rocinante\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This part can be left unchanged.\n",
    "## %load ../../standard_import.txt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## a package that does regression, so for comparison\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "## I write formulas in latex (what else?)\n",
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "pd.set_option('display.notebook_repr_html', False)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 150)\n",
    "pd.set_option('display.max_seq_items', None)\n",
    " \n",
    "%matplotlib inline  \n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a folder data with all data sets\n",
    "data = np.loadtxt('data/ex1data1.txt', delimiter=',')\n",
    "## we add a column in x with only 1s: \n",
    "## we need the constant function in our linear approximation\n",
    "## have a look at np.c is doing, e.g.,  \n",
    "## https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.c_.html\n",
    "X = np.c_[np.ones(data.shape[0]),data[:,0]]\n",
    "y = np.c_[data[:,1]]\n",
    "\n",
    "## BONUS: Once you have completed the entire notebook you can come back to this point to make a more complex model\n",
    "## ie. by adding higher order exponents p of data[:,0]**p to the feature vector X. You will probably run into\n",
    "## the problem of a diverging cost. How could this be solved? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on you should change the lines indicated with ... (and remove ## of course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a scatter plot of the data X versus y\n",
    "## Note that we added ones in the X array\n",
    "## on x-axis: Population of City in 10,000s\n",
    "## on y-axis: Profit in $10,000s\n",
    "##  plt..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $$ \\mbox{The cost function:} \\hspace{1cm} J = \\frac{1}{2M} \\sum_{m=0}^{M-1} \\left(\\sum_{\\ell=0}^1\\theta_\\ell X_{m, \\ell} -y_m\\right)^2 = \\frac{1}{2M} \\|X \\cdot \\theta - y\\|^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(X, y, theta):\n",
    "##  M = ...\n",
    "##  J = ...\n",
    "##  return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $$ \\mbox{The gradient descent:} \\hspace{1cm} \\theta \\to \\theta - \\frac{\\alpha}{M} X^T (X\\cdot \\theta -y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X, y, theta, alpha = 0.01, num_iters = 5500):\n",
    "    M = y.size\n",
    "    Jh = np.zeros(num_iters)\n",
    "    ## keep all data in Jh such that we can plot\n",
    "    for iter in np.arange(num_iters):\n",
    "        ## theta = ...\n",
    "        ## Jh[...] = ...\n",
    "    return(theta, Jh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## theta for minimized cost J \n",
    "## initializing theta \n",
    "theta = np.c_[np.zeros(X.shape[1])]\n",
    "print(' initial cost: ',computeCost(X,y,theta))\n",
    "theta , Jh = gradientDescent(X, y, theta)\n",
    "print('theta: ',theta[0], theta[1])\n",
    "## plot the history of the cost function\n",
    "## put sensible text on axis\n",
    "## plt ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compare with Scikit-learn Linear regression\n",
    "## https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "## you will see some boring python stuff here \n",
    "regr = LinearRegression()\n",
    "regr.fit(X[:,1].reshape(-1,1), y.ravel())\n",
    "print(regr.intercept_, regr.coef_)\n",
    "xx = np.arange(np.min(X[:,1]),np.max(X[:,1]))\n",
    "plt.plot(xx, regr.intercept_ + regr.coef_*xx, label='Linear regression (Scikit-learn GLM)')\n",
    "\n",
    "## add the (same) scatter plot and include also your regression line; add labels\n",
    "## plt ...\n",
    "\n",
    "plt.plot(xx, theta[0] + theta[1] * xx, label='Linear regression (Gradient descent)')\n",
    "plt.legend(loc=4);\n",
    "## what to do about the difference (albeit so small)??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict profit for a city with population of 35000 and 70000\n",
    "## print ...\n",
    "## print ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create grid coordinates for (3D) plotting J as a function of theta0, theta1\n",
    "## we define gridpoints around the optimal value \n",
    "## just study this code, it can be of later use !\n",
    "B0 = np.linspace(-10, 10, 50)\n",
    "B1 = np.linspace(-1, 4, 50)\n",
    "xx, yy = np.meshgrid(B0, B1, indexing='xy')\n",
    "Z = np.zeros((B0.size, B1.size))\n",
    "\n",
    "## Calculate Z-values (Cost) based on grid of coefficients\n",
    "for (i,j),v in np.ndenumerate(Z):\n",
    "    Z[i,j] = computeCost(X,y, theta=[[xx[i,j]], [yy[i,j]]])\n",
    "\n",
    "fig = plt.figure(figsize=(15,6))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "\n",
    "## Left plot\n",
    "CS = ax1.contour(xx, yy, Z, np.logspace(-2, 3, 20), cmap=plt.cm.jet)\n",
    "ax1.scatter(theta[0],theta[1], c='r')\n",
    "\n",
    "## Right plot\n",
    "ax2.plot_surface(xx, yy, Z, rstride=1, cstride=1, alpha=0.6, cmap=plt.cm.jet)\n",
    "ax2.set_zlabel('Cost')\n",
    "ax2.set_zlim(Z.min(),Z.max())\n",
    "ax2.view_init(elev=15, azim=230)\n",
    "\n",
    "## settings common to both plots\n",
    "for ax in fig.axes:\n",
    "    ax.set_xlabel(r'$\\theta_0$', fontsize=17)\n",
    "    ax.set_ylabel(r'$\\theta_1$', fontsize=17)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
